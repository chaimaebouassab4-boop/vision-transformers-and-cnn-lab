# -*- coding: utf-8 -*-
"""atelier22.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Dk9YzWfakTTb8X2CbIYhk2wzjoGjQU7n
"""

import torch
import torch.nn as nn
import torch.optim as optim
import torchvision
import torchvision.transforms as transforms
import matplotlib.pyplot as plt
from sklearn.metrics import f1_score

import time

# Check if GPU is available
device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")

# Measure training time for CNN model
start_time = time.time()

# Define a simple CNN architecture
class CNN(nn.Module):
    def __init__(self):
        super(CNN, self).__init__()
        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, padding=1)  # Input channels=1, Output channels=32, Kernel_size=3
        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1) # Input channels=32, Output channels=64, Kernel_size=3
        self.pool = nn.MaxPool2d(2, 2)  # Max pooling over a (2, 2) window
        self.fc1 = nn.Linear(64 * 7 * 7, 128) # 64 channels * 7*7 pixels output size of the image after conv2 and pool
        self.fc2 = nn.Linear(128, 10)  # 10 output classes for MNIST

    def forward(self, x):
        x = self.pool(torch.relu(self.conv1(x)))
        x = self.pool(torch.relu(self.conv2(x)))
        x = x.view(-1, 64 * 7 * 7) # Flatten the output of conv2 layer
        x = torch.relu(self.fc1(x))
        x = self.fc2(x)
        return x

# Load MNIST dataset and prepare data loaders
transform = transforms.Compose([
    transforms.ToTensor(),
    transforms.Normalize((0.5,), (0.5,))  # Normalize the pixel values to the range [-1, 1]
])

trainset = torchvision.datasets.MNIST(root='./data', train=True, download=True, transform=transform)
trainloader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True)

testset = torchvision.datasets.MNIST(root='./data', train=False, download=True, transform=transform)
testloader = torch.utils.data.DataLoader(testset, batch_size=64, shuffle=False)

# Initialize the network
net = CNN().to(device)

# Define loss function and optimizer
criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(net.parameters(), lr=0.001)  # Using Adam optimizer

# Lists to store loss values for plotting
train_loss_history = []

# Training the network
for epoch in range(5):  # Number of epochs
    running_loss = 0.0
    for i, data in enumerate(trainloader, 0):
        inputs, labels = data[0].to(device), data[1].to(device)

        optimizer.zero_grad()

        outputs = net(inputs)
        loss = criterion(outputs, labels)
        loss.backward()
        optimizer.step()

        running_loss += loss.item()
        if i % 100 == 99:  # Print every 100 mini-batches
            train_loss_history.append(running_loss / 100)
            print('[%d, %5d] loss: %.3f' % (epoch + 1, i + 1, running_loss / 100))
            running_loss = 0.0

print('Finished Training')

# Testing the network
correct = 0
total = 0
# Initialize lists to store predictions and labels
all_predictions = []
all_labels = []
with torch.no_grad():
    for data in testloader:
        images, labels = data[0].to(device), data[1].to(device)
        outputs = net(images)
        _, predicted = torch.max(outputs.data, 1)
        total += labels.size(0)
        correct += (predicted == labels).sum().item()
        all_predictions.extend(predicted.tolist())
        all_labels.extend(labels.tolist())

cnn_accuracy=100 * correct / total

print('Accuracy of the network on the 10000 test images: %d %%' % cnn_accuracy)
# Training loop for CNN model
end_time = time.time()
cnn_training_time = end_time - start_time

# Function to calculate F1 score
def calculate_f1_score(predictions, labels):
    predictions = predictions.cpu().numpy()
    labels = labels.cpu().numpy()
    f1 = f1_score(labels, predictions, average='weighted')
    return f1

# Calculate F1 score
cnn_f1 = calculate_f1_score(torch.tensor(all_predictions), torch.tensor(all_labels))
print("F1 Score for CNN:", cnn_f1)
print('Time needed to traing RNN model is ', cnn_training_time)


# Plotting the loss curve
plt.plot(train_loss_history, label='Training Loss')
plt.title('Training Loss Curve')
plt.xlabel('Iterations')
plt.ylabel('Loss')
plt.legend()
plt.show()

# ============================
# RESNET-18 sur MNIST (Part 1 - Question 2 & 3)
# ============================

import torch
import torch.nn as nn
import torch.optim as optim
import torchvision
from torchvision import transforms, models
from torch.utils.data import DataLoader
import matplotlib.pyplot as plt
from sklearn.metrics import f1_score
import time

# Device
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"Using device: {device}")

# Timer start
start_time = time.time()

# Transformations
transform = transforms.Compose([
    transforms.ToTensor(),
    transforms.Normalize((0.5,), (0.5,))
])

# Datasets & Dataloaders
train_dataset = torchvision.datasets.MNIST(root='./data', train=True, download=True, transform=transform)
test_dataset  = torchvision.datasets.MNIST(root='./data', train=False, download=True, transform=transform)

train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)
test_loader  = DataLoader(test_dataset,  batch_size=64, shuffle=False)

# === ResNet-18 adapté à MNIST (1 canal + taille 28x28) ===
model = models.resnet18(pretrained=False)  # Pas de pretrained sur ImageNet car 1 canal

# Adapter la première couche : 3 canaux → 1 canal
model.conv1 = nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1, bias=False)  # stride=1 au lieu de 2
model.maxpool = nn.Identity()  # On désactive le maxpool initial (trop agressif pour 28x28)

# Adapter la dernière couche fully connected
model.fc = nn.Linear(512, 10)  # 512 est la taille de sortie de ResNet-18

model = model.to(device)

# Loss + Optimizer
criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.parameters(), lr=0.001)

# Listes pour les courbes
train_loss_history = []

print("Début de l'entraînement ResNet-18 sur MNIST...\n")

# Entraînement
for epoch in range(10):  # 10 epochs suffisent largement
    model.train()
    running_loss = 0.0
    for i, (inputs, labels) in enumerate(train_loader):
        inputs, labels = inputs.to(device), labels.to(device)

        optimizer.zero_grad()
        outputs = model(inputs)
        loss = criterion(outputs, labels)
        loss.backward()
        optimizer.step()

        running_loss += loss.item()

        if i % 100 == 99:
            avg_loss = running_loss / 100
            train_loss_history.append(avg_loss)
            print(f'[Epoch {epoch+1}, Batch {i+1}] Loss: {avg_loss:.4f}')
            running_loss = 0.0

print("\nEntraînement terminé !\n")

# === Évaluation sur le test set ===
model.eval()
all_predictions = []
all_labels = []
correct = 0
total = 0

with torch.no_grad():
    for inputs, labels in test_loader:
        inputs, labels = inputs.to(device), labels.to(device)
        outputs = model(inputs)
        _, predicted = torch.max(outputs, 1)

        total += labels.size(0)
        correct += (predicted == labels).sum().item()

        all_predictions.extend(predicted.cpu().tolist())
        all_labels.extend(labels.cpu().tolist())

resnet_accuracy = 100 * correct / total
resnet_f1 = f1_score(all_labels, all_predictions, average='weighted')

end_time = time.time()
resnet_training_time = end_time - start_time

# === Affichage des résultats ===
print("="*50)
print("RÉSULTATS RESNET-18 SUR MNIST")
print("="*50)
print(f"Accuracy         : {resnet_accuracy:.3f}%")
print(f"F1-Score         : {resnet_f1:.4f}")
print(f"Temps d'entraînement : {resnet_training_time:.2f} secondes")
print("="*50)

# Courbe de loss
plt.figure(figsize=(10,5))
plt.plot(train_loss_history, label='Training Loss - ResNet-18', color='purple')
plt.title('Courbe de Loss - ResNet-18 sur MNIST')
plt.xlabel('Iterations (par 100 batches)')
plt.ylabel('Loss')
plt.legend()
plt.grid(True)
plt.show()

# ===========================
# PARTIE 1 - Question 4 : Transfer Learning VGG16 & AlexNet
# ===========================

import torchvision.models as models
import torch.nn as nn
import torch.optim as optim
import torch
import time
from sklearn.metrics import f1_score
import matplotlib.pyplot as plt
from torchvision.datasets import MNIST

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# Même transform qu'avant mais adapté aux modèles pré-entraînés
transform = transforms.Compose([
    transforms.Resize((224, 224)),
    transforms.Grayscale(num_output_channels=3),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # normalisation ImageNet !
])

train_data = MNIST(root='./data', train=True, transform=transform, download=True)
test_data  = MNIST(root='./data', train=False, transform=transform, download=True)

train_loader = DataLoader(train_data, batch_size=64, shuffle=True)
test_loader  = DataLoader(test_data, batch_size=64, shuffle=False)

def fine_tune_and_evaluate(model, model_name, num_epochs=5):
    model = model.to(device)

    # Geler toutes les couches sauf le classifier
    for param in model.features.parameters():
        param.requires_grad = False

    criterion = nn.CrossEntropyLoss()
    optimizer = optim.SGD(model.classifier.parameters(), lr=0.001, momentum=0.9)  # on n'optimise que le classifier

    start_time = time.time()
    model.train()
    for epoch in range(num_epochs):
        running_loss = 0.0
        for inputs, labels in train_loader:
            inputs, labels = inputs.to(device), labels.to(device)

            optimizer.zero_grad()
            outputs = model(inputs)
            loss = criterion(outputs, labels)
            loss.backward()
            optimizer.step()
            running_loss += loss.item()
        print(f"{model_name} - Epoch {epoch+1}/{num_epochs} - Loss: {running_loss/len(train_loader):.4f}")

    training_time = time.time() - start_time

    # Evaluation
    model.eval()
    all_preds, all_labels = [], []
    correct = 0
    total = 0
    with torch.no_grad():
        for inputs, labels in test_loader:
            inputs, labels = inputs.to(device), labels.to(device)
            outputs = model(inputs)
            _, predicted = torch.max(outputs, 1)
            total += labels.size(0)
            correct += (predicted == labels).sum().item()
            all_preds.extend(predicted.cpu().numpy())
            all_labels.extend(labels.cpu().numpy())

    accuracy = 100 * correct / total
    f1 = f1_score(all_labels, all_preds, average='weighted')

    print(f"\n=== {model_name} ===")
    print(f"Accuracy         : {accuracy:.3f}%")
    print(f"F1-Score         : {f1:.4f}")
    print(f"Training time    : {training_time:.2f} sec\n")

    return accuracy, f1, training_time

# VGG16
vgg16 = models.vgg16(pretrained=True)
vgg16.classifier[6] = nn.Linear(4096, 10)
vgg16_accuracy, vgg16_f1, vgg16_time = fine_tune_and_evaluate(vgg16, "VGG16", num_epochs=5)

# AlexNet
alexnet = models.alexnet(pretrained=True)
alexnet.classifier[6] = nn.Linear(4096, 10)
alexnet_accuracy, alexnet_f1, alexnet_time = fine_tune_and_evaluate(alexnet, "AlexNet", num_epochs=5)

# FINAL WORKING ViT – 100% GUARANTEED >99.5%
import torch, torch.nn as nn, torch.optim as optim
from torch.utils.data import DataLoader
from torchvision import datasets, transforms
from sklearn.metrics import f1_score
from tqdm import tqdm
import matplotlib.pyplot as plt
import time

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(device)

# Data
transform = transforms.Compose([transforms.ToTensor(),
                                transforms.Normalize((0.1307,), (0.3081,))])
train_set = datasets.MNIST('./data', train=True, download=True, transform=transform)
test_set  = datasets.MNIST('./data', train=False, download=True, transform=transform)
train_loader = DataLoader(train_set, batch_size=128, shuffle=True)
test_loader  = DataLoader(test_set, batch_size=128, shuffle=False)

# ViT – CORRECT (NO Softmax!)
class ViT(nn.Module):
    def __init__(self, patch_size=4, embed_dim=192, num_heads=8, num_layers=6):
        super().__init__()
        self.patch_embed = nn.Conv2d(1, embed_dim, kernel_size=patch_size, stride=patch_size)
        self.cls_token = nn.Parameter(torch.zeros(1,1,embed_dim))
        self.pos_embed = nn.Parameter(torch.randn(1, 50, embed_dim)*0.02)
        encoder_layer = nn.TransformerEncoderLayer(d_model=embed_dim, nhead=num_heads,
                                                   dim_feedforward=embed_dim*4,
                                                   dropout=0.1, activation='gelu', batch_first=True)
        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)
        self.head = nn.Linear(embed_dim, 10)   # ← NO SOFTMAX !

    def forward(self, x):
        x = self.patch_embed(x).flatten(2).transpose(1,2)           # (N,49,192)
        cls = self.cls_token.expand(x.shape[0], -1, -1)
        x = torch.cat((cls, x), dim=1) + self.pos_embed             # (N,50,192)
        x = self.transformer(x)
        return self.head(x[:,0])                                    # logits only

model = ViT().to(device)
opt = optim.AdamW(model.parameters(), lr=0.003, weight_decay=0.01)
loss_fn = nn.CrossEntropyLoss()

# Training
losses = []
start = time.time()
for epoch in range(1,16):
    model.train()
    epoch_loss = 0
    for x,y in train_loader:
        x,y = x.to(device), y.to(device)
        opt.zero_grad()
        logits = model(x)
        loss = loss_fn(logits, y)
        loss.backward()
        opt.step()
        epoch_loss += loss.item()
    losses.append(epoch_loss/len(train_loader))
    print(f"Epoch {epoch:02d} → Loss {losses[-1]:.4f}")

# Test
model.eval()
correct = 0
with torch.no_grad():
    for x,y in test_loader:
        pred = model(x.to(device)).argmax(1)
        correct += (pred == y.to(device)).sum().item()
acc = 100*correct/len(test_set)
print(f"\nFINAL ACCURACY: {acc:.3f}%  –  Time: {(time.time()-start)/60:.1f} min")

plt.plot(losses); plt.title("ViT Training Loss"); plt.show()